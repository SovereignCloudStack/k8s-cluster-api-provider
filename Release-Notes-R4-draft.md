# Release Notes of SCS k8s-capi-provider for R4

k8s-cluster-api-provider was provided with Release 1 (R1) of Sovereign
Cloud Stack and has since seen major updates in R2, was
hardened for production use during the R3 development phase
and received a lot of real-world exposure since:

The SCS cluster management solution is heavily used by the
development and integration work in the [Gaia-X Federation 
Services (GXFS)](https://gxfs.eu/) project; the resulting
Open Source Software nicely combines with
[Sovereign Cloud Stack](https://scs.community/) to form a
rather complete set of tools that can be used to provide
Gaia-X conforming services on top of sovereign infrastructure.

R4 was released on 2024-03-22.

## Updated software

### capi v1. and openstack capi provider 0.

[Kubernetes Cluster API Provider](https://cluster-api.sigs.k8s.io/)
[OpenStack Provider for CAPI](https://cluster-api-openstack.sigs.k8s.io/)

### k8s versions (1.22 -- 1.26)

We test the Kubernetes versions 1.22 -- 1.26 with the R3 Cluster API
solution. We had tested earlier versions (down to 1.18) successfully before,
and we don't expect them to break, but these are no longer supported
upstream and no fresh node images are provided by us.

Please note that k8s-v1.25 brought the removal of the deprecated Pod Security
Policies (PSPs) and brought  
[Pod Security Standards (PSS)](https://kubernetes.io/blog/2022/08/25/pod-security-admission-stable/) 
instead.

k8s-v1.26 is not officially supported by capi yet; it has survived
the CNCF testsuite, rolling upgrades and `clusterctl move`s though,
so we do allow the deployment using an override parameter.

### calico 3..x, cilium 1..x, helm 3.11.x, sonobuoy 0..x, k9s 0..x, kind 0.17.1

We regularly update to the latest stable versions.

### cert-manager 1..x, nginx-ingress 1..0

## New features

### Completed upgrade guide (#293)

See `doc/` directory.
<https://github.com/SovereignCloudStack/k8s-cluster-api-provider/blob/main/doc/Upgrade-Guide.md>

### Completed maintenance and troubleshooting guide (#292)

Please check the doc directory.
<https://github.com/SovereignCloudStack/k8s-cluster-api-provider/blob/main/doc/Maintenance_and_Troubleshooting.md>

## Changed defaults/settings

## Important Bugfixes

### containers moved from k8s.gcr.io to registry.k8s.io
....

## Upgrade/Migration notes

### Incompatible changes

## Removals and deprecations

Please note that the `ETCD_PRIO_BOOST` setting has been removed;
it was deprecated in R3 and had been ignored there already.
No breakage.

## Known issues and limitations

### Clusters need maintenance

Like with other kubeadm k8s clusters, there is some maintenance required for the
cluster operators. The client certificates used by the various k8s components
have a 1-year lifetime. We have ensured that certificate rotation is enabled;
upon an update (e.g. doing a patchlevel k8s version upgrade), the rotation
will happen. However, if left alone for more than a year, the certificates
will expire and the cluster will need some loving care then to be revived
with `kubeadm certs rotate` on the control-plane nodes. Please see the
Maintenance and Troubleshooting Guide for more details.

We consider adding a job on the control-nodes to avoid this in a future release.

Please be aware that the CA generated by k8s expires after 10 years;
if you intend to run clusters for longer, please be aware of this. An external
CA might be a good idea (see next paragraph).

### metrics with --kubelet-insecure-tls (#148)

Like most kubeadm based setups, we used --kubelet-insecure-tls for the metric
service to be allowed to talk to kubelets to retrieve metrics. This can be improved
by using a CA-signed server cert for the kubelets.
We have some thoughts on using a CA external to the control-plane nodes and
have thus not addressed this yet.

### No removal of services from `create_cluster.sh` (#137)

You can call `create_cluster.sh` many times to apply changes to your
workload cluster -- it is idempotent and does not do any changes if your config
is unchanged. If you enabled extra services, these will get deployed.
It currently however does not remove any of the deployed
standard services that you might have had enabled before and now set to
`false`. (We will require a `--force-remove` option or so to ensure that
users are aware of the risks.) This is unchanged from R2.

### No support for changing b/w calico and cilium (#131)

Switching between the two CNI alternatives is currently not facilitated
by the `create_cluster.sh` script. It can be done by removing the
current CNI manually (delete the deployment yaml or cilium uninstall)
and then calling `create_cluster.sh`. However, this has the potential
to disrupt the workloads in your workload clusters.
This is unchanged from R2.

### One CNCF conformance test fail with cilium (#144)

We want to understand whether this failure could be avoided by tweaking
the configuration or whether those are commonly ignored. More investigation
is needed.
```
[sig-network] HostPort validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly]
 [Conformance]
```
This is down from four failures in R2.

## Future roadmap

### Rate limiting 

To protect the etcd from overflowing, we have enabled compaction and defragmentation.
It is still possible to overwhelm etcd by firing k8s API calls at crazy rates.
It is best practice to enable rate-limiting at the kubeapi level, which we intend
to do after R3 (as opt-in feature -- it might become default in R4).

### Access controls

By default, the kubeapi server exposes the k8s API to the public internet via a
load balancer. While this interface is well protected, it is still a level of
exposure that security-aware people tend to dislike. So we plan to allow limiting
the access to be only available internally (i.e. from the cluster itself,
the management host and an optional bastion host) plus selected IP ranges that
the user specifies. This will be an opt-in feature and we plan to deliver it
prior to R4.

### harbor (#139)

We have a harbor registry for hosting (and scanning) image artifacts
for the SCS community. This has been built using the 
[SCS k8s-harbor](https://github.com/SovereignCloudStack/k8s-harbor) repository.
We intend to provide an easy way to create ow harbor instances along with
SCS cluster management.

### Cluster standardization

Most users of the cluster-API based cluster management will only ever need
to touch the `clusterctl.yaml` settings file. Our intention is thus to
standardize cluster-management on that level: Have a simple yaml file
that describes the wanted cluster state. Exposing all the power of
cluster-API behind it is optional and not required for SCS conformance.

The settings are currently processed with the cluster-template by clusterctl
and then submitted to the k8s management cluster. In our to be standardized
approach, we will have a few cluster templates; the simple one that is
already used today and more complex ones that e.g. support multiple machine
deployments. The settings will be made cloud-provider independent; we intend
to allow non-OpenStack clouds and even non-CAPI implementations with respects
to SCS standards conformance. Obviously, few pieces of the reference implementation
will work as is, but this should not affect the user. The cluster-templates
obviously will be provider dependent as well, but its behavior will be
standardized.

To allow for templating, we may go beyond the clusterctl capabilities
and use helm or helmfile for this. This may also allow us to incorporate
some of the nice work from the
[capi-helm-charts](https://github.com/stackhpc/capi-helm-charts) from
[StackHPC](https://stackhpc.com).

We are currently pondering whether we can expose the k8s management cluster
kube API to users in a multi-tenant scenario. We certainly would need some
work with namespaces, kata containers and such to make it safe in a
multi-tenant scenario. Right now, we may opt to put a REST interface
in front of the kubeAPI to better shield it.

We had some thoughts to allow gitops style management
(see [Docs/#47](https://github.com/SovereignCloudStack/Docs/pull/47))
where cluster settings
would be automatically fed from a git repository; we still have this vision,
but after numerous discussions came to the conclusion that this will be
an opt-in feature.

## Conformance

With Calico CNI, the k8s clusters created with our SCS cluster-API based
cluster management solution pass the CNCF conformance tests as run by
[sonobuoy](https://sonobuoy.io/).

With the gitops approach, we intend to standardize the
`clusterctl.yaml` settings to allow a straightforward approach to
declarative cluster management. This is intended for R4 (3/2023).

## Branching

We tag the R4 branch with the `v5.0.0` tag and create a 
`maintained/v5.0.x` branch for users that want to exclusively see bug
and security fixes. We will also create a `maintained/v5.x` branch for
minor releases (which however might never see anything beyond what
we put into v5.0.x if we don't create a minor release). 
If we decide to create a minor release, we would also create a 
v5.1.0 tag and a v5.1.x branch.
These branches will receive updates until the end of April 2023.

## Contribution

We appreciate contribution to strategy and implemention, please join
our community -- or just leave input on the github issues and PRs.
Have a look at our [contribution invitation](https://scs.community/contribute/).
